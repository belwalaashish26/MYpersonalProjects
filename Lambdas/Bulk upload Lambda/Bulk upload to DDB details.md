ğŸ“¥ S3 â†’ DynamoDB CSV Loader (Lambda)

This AWS Lambda function ingests CSV employee records, converts them into structured objects, and stores them in DynamoDB.
It supports three execution modes: Warmup, Test input, and S3-triggered ingestion.

ğŸ”‘ Key Responsibilities

âœ”ï¸ Reads CSV rows
âœ”ï¸ Converts emp_id to a numeric PK
âœ”ï¸ Performs bulk inserts using DynamoDB batch_writer
âœ”ï¸ Logs processing status, successes, and failures

ğŸš€ Execution Modes
1ï¸âƒ£ Warm-up / Pre-initialization

Used to keep the Lambda â€œhotâ€ (e.g., via CloudWatch).

{"warmup": true}


ğŸ”¹ Returns immediately without processing data.

2ï¸âƒ£ Test Mode (Manual Invocation)

Pass CSV-like rows directly when testing in Lambda console:

{
  "test": true,
  "rows": [
    {"emp_id": "101", "name": "John", "dept": "IT"},
    {"emp_id": "102", "name": "Emma", "dept": "HR"}
  ]
}


ğŸ”¹ No S3 required
ğŸ”¹ Useful for debugging or local PoC

3ï¸âƒ£ S3 Trigger Mode (Main)

Triggered by an S3 event when a .csv is uploaded.

Flow:

Extract bucket and file key from the event

Download the CSV

Convert rows â†’ Python dicts

Insert items in DynamoDB

Example event source:

S3 Object Created trigger (Put / Upload)

ğŸ—„ï¸ DynamoDB

Table name supplied via environment variable:

DYNAMO_TABLE_NAME=*****


Partition Key must be:

emp_id (Number)

Every CSV row becomes a DynamoDB item.

Example saved item:

{
  "emp_id": 101,
  "name": "John",
  "dept": "IT",
  ...
}

ğŸ“¦ CSV Input Format

CSV headers should match field names (case-sensitive).

Example:

emp_id,name,dept,location
101,John,IT,Bangalore
102,Emma,HR,Mumbai

ğŸ§¾ Processing Rules
âœ”ï¸ emp_id

Required

Must be a numeric value

Converted to int before storing

âœ”ï¸ Other Fields

Stored as-is from CSV

No transformation applied

âŒ Invalid Rows

Missing or non-numeric emp_id â†’ skipped

Logged and counted in failed

ğŸ”§ Logging

The Lambda logs:

Startup

Trigger mode detected

DynamoDB writes

Warnings for invalid rows

Useful for debugging and tracking ingestion quality.

ğŸ› ï¸ AWS Services Used
Service	Purpose
AWS Lambda	Ingestion + processing
S3	CSV storage & event trigger
DynamoDB	Employee records storage
Boto3	AWS SDK to interact with S3/DynamoDB
ğŸ” IAM Permissions (Minimum)

s3:GetObject

dynamodb:PutItem

dynamodb:BatchWriteItem

logs:CreateLogGroup

logs:CreateLogStream

logs:PutLogEvents

ğŸ” Return Format

The response includes success & failure counts:

{
  "inserted": 125,
  "failed": 3
}

ğŸ§ª Testing Strategy

âœ”ï¸ Use Test Mode for local validation
âœ”ï¸ Upload sample CSVs to S3 buckets
âœ”ï¸ Validate DynamoDB writes
âœ”ï¸ Monitor CloudWatch logs for skipped rows

ğŸ’¡ Notes

No schema enforcement besides emp_id.

All CSV columns are dynamically mapped.

Batch writer optimizes write throughput but does not retry per item.